{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41c566de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import Dense, Dropout, BatchNormalization,Input\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras import regularizers,initializers\n",
    "from keras.layers import GaussianNoise\n",
    "import keras.backend as Kb\n",
    "import keras.losses\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.utils import class_weight\n",
    "import numpy as np\n",
    "from numpy import exp\n",
    "# Library for Gaussian process\n",
    "# import GPy\n",
    "##Library for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib;matplotlib.rcParams['figure.figsize'] = (8,6)\n",
    "import pylab \n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2f23b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Basis functions\n",
    "\n",
    "num_basis = [3**2,5**2,9**2]\n",
    "knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "##Wendland kernel\n",
    "K = 0\n",
    "phi = np.zeros((N, sum(num_basis)))\n",
    "for res in range(len(num_basis)):\n",
    "    theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "    knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "    knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "    for i in range(num_basis[res]):\n",
    "        d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "        for j in range(len(d)):\n",
    "            if d[j] >= 0 and d[j] <= 1:\n",
    "                phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "            else:\n",
    "                phi[j,i + K] = 0\n",
    "    K = K + num_basis[res]\n",
    "\n",
    "### Train Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aeccfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### pre-processing (multiply with lmc estimates)\n",
    "\n",
    "phi1 = 1.28*phi + 0.36*phi   ### variable 1 basis functions\n",
    "phi2 = 0.639*phi + 0.903*phi ### variable 2 basis functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8671b04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Functions for calculation of MSE and MAE\n",
    "def mse(y_pred,y_true):\n",
    "    mse = np.mean((y_pred-y_true)**2)\n",
    "    return mse\n",
    "\n",
    "def mae(y_pred,y_true):\n",
    "    mae = np.mean(np.absolute(y_pred-y_true))\n",
    "    return mae\n",
    "\n",
    "mse_var1 = []\n",
    "mse_var2 = []\n",
    "correlation = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65903231",
   "metadata": {},
   "source": [
    "## Independent DeepKriging with replicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b36cf572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300\n",
      "13/13 - 1s - loss: 1.3415 - mae: 0.6128 - mse: 1.3234 - val_loss: 0.8060 - val_mae: 0.5588 - val_mse: 0.7886\n",
      "Epoch 2/300\n",
      "13/13 - 0s - loss: 0.8211 - mae: 0.5110 - mse: 0.8039 - val_loss: 0.3845 - val_mae: 0.3778 - val_mse: 0.3676\n",
      "Epoch 3/300\n",
      "13/13 - 0s - loss: 0.5364 - mae: 0.4277 - mse: 0.5196 - val_loss: 0.2402 - val_mae: 0.2909 - val_mse: 0.2236\n",
      "Epoch 4/300\n",
      "13/13 - 0s - loss: 0.3342 - mae: 0.3401 - mse: 0.3178 - val_loss: 0.1720 - val_mae: 0.2538 - val_mse: 0.1557\n",
      "Epoch 5/300\n",
      "13/13 - 0s - loss: 0.2373 - mae: 0.2713 - mse: 0.2211 - val_loss: 0.1552 - val_mae: 0.2287 - val_mse: 0.1392\n",
      "Epoch 6/300\n",
      "13/13 - 0s - loss: 0.1805 - mae: 0.2426 - mse: 0.1646 - val_loss: 0.1613 - val_mae: 0.2449 - val_mse: 0.1455\n",
      "Epoch 7/300\n",
      "13/13 - 0s - loss: 0.1422 - mae: 0.2378 - mse: 0.1265 - val_loss: 0.1524 - val_mae: 0.2162 - val_mse: 0.1368\n",
      "Epoch 8/300\n",
      "13/13 - 0s - loss: 0.1260 - mae: 0.2140 - mse: 0.1105 - val_loss: 0.1456 - val_mae: 0.2216 - val_mse: 0.1303\n",
      "Epoch 9/300\n",
      "13/13 - 0s - loss: 0.2053 - mae: 0.2438 - mse: 0.1900 - val_loss: 0.1453 - val_mae: 0.2173 - val_mse: 0.1301\n",
      "Epoch 10/300\n",
      "13/13 - 0s - loss: 0.1416 - mae: 0.2277 - mse: 0.1265 - val_loss: 0.1382 - val_mae: 0.2365 - val_mse: 0.1232\n",
      "Epoch 11/300\n",
      "13/13 - 0s - loss: 0.1034 - mae: 0.1993 - mse: 0.0885 - val_loss: 0.1263 - val_mae: 0.2164 - val_mse: 0.1115\n",
      "Epoch 12/300\n",
      "13/13 - 0s - loss: 0.0787 - mae: 0.1694 - mse: 0.0639 - val_loss: 0.1440 - val_mae: 0.2155 - val_mse: 0.1293\n",
      "Epoch 13/300\n",
      "13/13 - 0s - loss: 0.1046 - mae: 0.1854 - mse: 0.0899 - val_loss: 0.1096 - val_mae: 0.1869 - val_mse: 0.0951\n",
      "Epoch 14/300\n",
      "13/13 - 0s - loss: 0.0774 - mae: 0.1662 - mse: 0.0630 - val_loss: 0.1199 - val_mae: 0.1950 - val_mse: 0.1055\n",
      "Epoch 15/300\n",
      "13/13 - 0s - loss: 0.0901 - mae: 0.1775 - mse: 0.0757 - val_loss: 0.1128 - val_mae: 0.1824 - val_mse: 0.0985\n",
      "Epoch 16/300\n",
      "13/13 - 0s - loss: 0.0705 - mae: 0.1545 - mse: 0.0562 - val_loss: 0.1059 - val_mae: 0.1731 - val_mse: 0.0917\n",
      "Epoch 17/300\n",
      "13/13 - 0s - loss: 0.0743 - mae: 0.1638 - mse: 0.0602 - val_loss: 0.1301 - val_mae: 0.1846 - val_mse: 0.1160\n",
      "Epoch 18/300\n",
      "13/13 - 0s - loss: 0.0790 - mae: 0.1675 - mse: 0.0650 - val_loss: 0.1387 - val_mae: 0.2052 - val_mse: 0.1247\n",
      "Epoch 19/300\n",
      "13/13 - 0s - loss: 0.0855 - mae: 0.1735 - mse: 0.0715 - val_loss: 0.1069 - val_mae: 0.1793 - val_mse: 0.0930\n",
      "Epoch 20/300\n",
      "13/13 - 0s - loss: 0.0972 - mae: 0.1827 - mse: 0.0833 - val_loss: 0.1083 - val_mae: 0.1928 - val_mse: 0.0945\n",
      "Epoch 21/300\n",
      "13/13 - 0s - loss: 0.0807 - mae: 0.1805 - mse: 0.0670 - val_loss: 0.1045 - val_mae: 0.1908 - val_mse: 0.0908\n",
      "Epoch 22/300\n",
      "13/13 - 0s - loss: 0.0701 - mae: 0.1616 - mse: 0.0564 - val_loss: 0.1081 - val_mae: 0.1809 - val_mse: 0.0944\n",
      "Epoch 23/300\n",
      "13/13 - 0s - loss: 0.0639 - mae: 0.1461 - mse: 0.0502 - val_loss: 0.1104 - val_mae: 0.1736 - val_mse: 0.0968\n",
      "Epoch 24/300\n",
      "13/13 - 0s - loss: 0.0641 - mae: 0.1457 - mse: 0.0506 - val_loss: 0.0965 - val_mae: 0.1692 - val_mse: 0.0830\n",
      "Epoch 25/300\n",
      "13/13 - 0s - loss: 0.0539 - mae: 0.1394 - mse: 0.0404 - val_loss: 0.0984 - val_mae: 0.1763 - val_mse: 0.0849\n",
      "Epoch 26/300\n",
      "13/13 - 0s - loss: 0.0513 - mae: 0.1342 - mse: 0.0378 - val_loss: 0.0984 - val_mae: 0.1686 - val_mse: 0.0850\n",
      "Epoch 27/300\n",
      "13/13 - 0s - loss: 0.0549 - mae: 0.1373 - mse: 0.0415 - val_loss: 0.0996 - val_mae: 0.1716 - val_mse: 0.0862\n",
      "Epoch 28/300\n",
      "13/13 - 0s - loss: 0.0519 - mae: 0.1349 - mse: 0.0386 - val_loss: 0.0995 - val_mae: 0.1707 - val_mse: 0.0862\n",
      "Epoch 29/300\n",
      "13/13 - 0s - loss: 0.0504 - mae: 0.1322 - mse: 0.0372 - val_loss: 0.1012 - val_mae: 0.1732 - val_mse: 0.0879\n",
      "Epoch 30/300\n",
      "13/13 - 0s - loss: 0.0514 - mae: 0.1330 - mse: 0.0382 - val_loss: 0.0932 - val_mae: 0.1631 - val_mse: 0.0800\n",
      "Epoch 31/300\n",
      "13/13 - 0s - loss: 0.0473 - mae: 0.1267 - mse: 0.0342 - val_loss: 0.0913 - val_mae: 0.1604 - val_mse: 0.0782\n",
      "Epoch 32/300\n",
      "13/13 - 0s - loss: 0.0511 - mae: 0.1327 - mse: 0.0380 - val_loss: 0.0951 - val_mae: 0.1694 - val_mse: 0.0820\n",
      "Epoch 33/300\n",
      "13/13 - 0s - loss: 0.0555 - mae: 0.1350 - mse: 0.0425 - val_loss: 0.1065 - val_mae: 0.1709 - val_mse: 0.0935\n",
      "Epoch 34/300\n",
      "13/13 - 0s - loss: 0.0554 - mae: 0.1383 - mse: 0.0424 - val_loss: 0.1086 - val_mae: 0.1673 - val_mse: 0.0956\n",
      "Epoch 35/300\n",
      "13/13 - 0s - loss: 0.0539 - mae: 0.1367 - mse: 0.0409 - val_loss: 0.0958 - val_mae: 0.1646 - val_mse: 0.0829\n",
      "Epoch 36/300\n",
      "13/13 - 0s - loss: 0.0531 - mae: 0.1298 - mse: 0.0402 - val_loss: 0.0995 - val_mae: 0.1729 - val_mse: 0.0866\n",
      "Epoch 37/300\n",
      "13/13 - 0s - loss: 0.0447 - mae: 0.1217 - mse: 0.0319 - val_loss: 0.0872 - val_mae: 0.1614 - val_mse: 0.0744\n",
      "Epoch 38/300\n",
      "13/13 - 0s - loss: 0.0475 - mae: 0.1297 - mse: 0.0346 - val_loss: 0.0935 - val_mae: 0.1760 - val_mse: 0.0807\n",
      "Epoch 39/300\n",
      "13/13 - 0s - loss: 0.0463 - mae: 0.1280 - mse: 0.0335 - val_loss: 0.1110 - val_mae: 0.1717 - val_mse: 0.0982\n",
      "Epoch 40/300\n",
      "13/13 - 0s - loss: 0.0445 - mae: 0.1217 - mse: 0.0318 - val_loss: 0.0865 - val_mae: 0.1554 - val_mse: 0.0738\n",
      "Epoch 41/300\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-07cb762b70b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mae'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'mse'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m     result = model.fit(encoder_train, y_train[:,0], \n\u001b[0m\u001b[1;32m     73\u001b[0m                        validation_data=(encoder_test,y_test[:,0]), epochs = 300, batch_size = 64, verbose = 2)\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1129\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m                 steps_per_execution=self._steps_per_execution)\n\u001b[0;32m-> 1131\u001b[0;31m           val_logs = self.evaluate(\n\u001b[0m\u001b[1;32m   1132\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1133\u001b[0m               \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1381\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_test_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_test_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1383\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Single epoch.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1384\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1385\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m       \u001b[0mdata_iterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 422\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    423\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m       raise RuntimeError(\"__iter__() is only supported inside of tf.function \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcomponents\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0melement_spec\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 682\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_create_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    703\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[0;32m--> 705\u001b[0;31m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m       \u001b[0;31m# Delete the resource when this object is deleted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[0;34m(dataset, iterator, name)\u001b[0m\n\u001b[1;32m   2969\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2970\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2971\u001b[0;31m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[1;32m   2972\u001b[0m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[1;32m   2973\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "## Functions for calculation of MSE and MAE\n",
    "def mse(y_pred,y_true):\n",
    "    mse = np.mean((y_pred-y_true)**2)\n",
    "    return mse\n",
    "\n",
    "def mae(y_pred,y_true):\n",
    "    mae = np.mean(np.absolute(y_pred-y_true))\n",
    "    return mae\n",
    "\n",
    "mse_var1 = []\n",
    "mse_var2 = []\n",
    "for i in range(1):\n",
    "    df_loc = pd.read_csv(\"../synthetic_data_simulations/2d_gaussian_1200_\"+str(i+1)+\".csv\", sep = \",\")\n",
    "\n",
    "    N = len(df_loc)\n",
    "    s = np.vstack((df_loc[\"x\"],df_loc[\"y\"])).T\n",
    "    y = np.array(df_loc[[\"var1\",\"var2\"]])\n",
    "    num_basis = [2**2,5**2,9**2]\n",
    "    knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "    #knots_1d = [np.linspace(0,1,i) for i in num_basis]\n",
    "    ##Wendland kernel\n",
    "    K = 0\n",
    "    phi = np.zeros((N, sum(num_basis)))\n",
    "    for res in range(len(num_basis)):\n",
    "        theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "        knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "        knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "        for i in range(num_basis[res]):\n",
    "            d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "            for j in range(len(d)):\n",
    "                if d[j] >= 0 and d[j] <= 1:\n",
    "                    phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "                else:\n",
    "                    phi[j,i + K] = 0\n",
    "        K = K + num_basis[res]\n",
    "    s_train, s_test, encoder_train, encoder_test    , y_train, y_test= train_test_split(s, phi, y, \n",
    "                                                                                    test_size=0.3333)\n",
    "    N_train = s_train.shape[0]\n",
    "    N_test = s_test.shape[0]\n",
    "\n",
    "\n",
    "    # DeepKriging model for continuous data\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(100, input_dim = encoder_train.shape[1],  \n",
    "                kernel_initializer=initializers.RandomNormal(stddev=0.01), activation='relu'))\n",
    "    #     model.add(Dense(100, input_dim = encoder_train.shape[1],  kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(rate=0.5))\n",
    "    # model.add(BatchNormalization())\n",
    "    #     model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "                bias_regularizer=regularizers.l2(1e-4),\n",
    "                activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
    "    #     model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    #     model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    # model.add(Dense(100, activation='relu'))\n",
    "    #model.add(Dropout(rate=0.5))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(1, activation='linear'))\n",
    "    NB_START_EPOCHS = 50 \n",
    "    # NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "    BATCH_SIZE = 64  \n",
    "    fold_no = 1\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae','mse'])\n",
    "\n",
    "    result = model.fit(encoder_train, y_train[:,0], \n",
    "                       validation_data=(encoder_test,y_test[:,0]), epochs = 300, batch_size = 64, verbose = 2)\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=40),\n",
    "                 ModelCheckpoint(filepath='Biv_gaussian.h5', monitor='val_loss', save_best_only=True)]\n",
    "    result = model.fit(encoder_train, y_train[:,0], callbacks=callbacks, \n",
    "                       validation_data=(encoder_test,y_test[:,0]), epochs = 250, batch_size = 64, verbose = 2)\n",
    "    model = keras.models.load_model('Biv_gaussian.h5')\n",
    "    y_pred1 = model.predict(encoder_test)\n",
    "\n",
    "#     model = Sequential()\n",
    "#     # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "#     model.add(Dense(100, input_dim = encoder_train.shape[1],  \n",
    "#                 kernel_initializer=initializers.RandomNormal(stddev=0.01), activation='relu'))\n",
    "#     #     model.add(Dense(100, input_dim = encoder_train.shape[1],  kernel_initializer='he_uniform', activation='relu'))\n",
    "#     # model.add(Dropout(rate=0.5))\n",
    "#     # model.add(BatchNormalization())\n",
    "#     #     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, kernel_regularizer=regularizers.L1L2(l1=1e-5, l2=1e-4),\n",
    "#                 bias_regularizer=regularizers.l2(1e-4),\n",
    "#                 activity_regularizer=regularizers.l2(1e-5),activation='relu'))\n",
    "#     #     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     #     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     # model.add(Dense(100, activation='relu'))\n",
    "#     #model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     #model.add(BatchNormalization())\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "#     NB_START_EPOCHS = 50 \n",
    "#     # NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "#     BATCH_SIZE = 64  \n",
    "#     fold_no = 1\n",
    "#     optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "#     model.compile(optimizer=optimizer, loss='mse', metrics=['mae','mse'])\n",
    "\n",
    "#     result = model.fit(encoder_train, y_train[:,1], \n",
    "#                        validation_data=(encoder_test,y_test[:,1]), epochs = 300, batch_size = 64, verbose = 2)\n",
    "\n",
    "#     callbacks = [EarlyStopping(monitor='val_loss', patience=40),\n",
    "#                  ModelCheckpoint(filepath='Biv_gaussian.h5', monitor='val_loss', save_best_only=True)]\n",
    "#     result = model.fit(encoder_train, y_train[:,1], callbacks=callbacks, \n",
    "#                        validation_data=(encoder_test,y_test[:,1]), epochs = 250, batch_size = 64, verbose = 2)\n",
    "# #     model = keras.models.load_model('Biv_gaussian.h5')\n",
    "#     y_pred2 = model.predict(encoder_test)\n",
    "\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse_var1.append(mse(y_pred1, y_test[:,0]))\n",
    "#     mse_var2.append(mse(y_pred2, y_test[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "27c2bc21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.6596804632087576]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_var1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "55811d88",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 1 is out of bounds for axis 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-6aa324386e8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my_pred1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 1 is out of bounds for axis 1 with size 1"
     ]
    }
   ],
   "source": [
    "y_pred1[:,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11bd1d47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.35192119e+00, -1.11566737e-01],\n",
       "       [ 3.24607966e-01,  9.10878497e-01],\n",
       "       [-2.60916552e+00,  1.19471810e+00],\n",
       "       [-1.29854368e+00,  2.72833636e+00],\n",
       "       [-6.36592760e-01,  7.09306462e-01],\n",
       "       [-1.72179499e-02,  5.71126932e-01],\n",
       "       [ 4.70802421e-02,  3.67449608e-01],\n",
       "       [-5.02442459e-01,  2.19964194e-01],\n",
       "       [-2.10049470e-01,  1.57944606e-01],\n",
       "       [ 1.60019299e-02,  3.53711780e-01],\n",
       "       [ 1.13474809e-01,  4.35573618e+00],\n",
       "       [-4.62000323e-01,  4.57175542e+00],\n",
       "       [ 1.23258357e+00,  1.00112497e+00],\n",
       "       [-5.43343646e-01, -1.04736761e-01],\n",
       "       [-1.54091296e-01,  3.82301367e-01],\n",
       "       [-2.79138565e-01,  1.14270739e+00],\n",
       "       [-5.48104288e-02,  3.40096470e+00],\n",
       "       [-2.71982777e+00,  9.30134428e-01],\n",
       "       [ 2.35339826e-02,  4.22769062e+00],\n",
       "       [-3.97782934e-01,  1.07433095e+00],\n",
       "       [-1.37552085e+00, -3.59828608e-01],\n",
       "       [-4.12751149e-01, -5.01662932e-02],\n",
       "       [-2.87065727e-01,  1.15146103e+00],\n",
       "       [-1.06766611e-01,  2.94451570e-01],\n",
       "       [-1.23814433e+00, -4.56147005e-01],\n",
       "       [-5.26043136e-02,  2.29000954e-01],\n",
       "       [-1.16534474e+00,  1.43874160e+00],\n",
       "       [-4.48356976e-01, -2.19114683e-01],\n",
       "       [-1.22270412e-01,  2.11932672e-01],\n",
       "       [-5.61307765e+00, -2.39947895e-01],\n",
       "       [-2.58415563e-01,  4.20870147e-01],\n",
       "       [-4.85348452e-01, -1.48846174e-01],\n",
       "       [-2.04040548e-02,  1.07867039e+00],\n",
       "       [-1.94103607e+00,  6.51023030e-01],\n",
       "       [ 5.77026481e-01,  3.56965768e-01],\n",
       "       [ 7.05418236e-01,  1.09949447e+00],\n",
       "       [-6.18371651e-01,  7.41608762e-02],\n",
       "       [-1.51486113e-01,  2.37224811e-01],\n",
       "       [-2.41675558e-01,  4.84500762e-01],\n",
       "       [ 1.78123588e-01,  5.58912270e-01],\n",
       "       [-8.34157718e-02,  1.36657377e-01],\n",
       "       [-1.42052230e-01,  2.48954055e+00],\n",
       "       [-4.40432388e+00, -5.27939915e-01],\n",
       "       [-1.99247072e-01,  1.51273834e+00],\n",
       "       [ 7.13399558e-02,  1.25907705e+00],\n",
       "       [-2.20239089e-01,  1.12728242e+00],\n",
       "       [-2.15541275e+00, -1.29496469e-01],\n",
       "       [-4.33308793e-01, -1.84272550e-01],\n",
       "       [-1.41118662e+00,  9.34001087e-01],\n",
       "       [ 8.62773511e-01,  1.12146024e+00],\n",
       "       [-1.22591406e-01,  2.79220309e+00],\n",
       "       [ 1.41333175e+00,  1.76141970e+00],\n",
       "       [ 1.88499344e-01,  1.61602108e+00],\n",
       "       [-7.52752223e-01, -1.98672077e-01],\n",
       "       [-6.31990754e-01,  1.08128125e+00],\n",
       "       [-4.08334326e-01,  1.38011918e+00],\n",
       "       [ 6.92666485e-02,  3.62982618e+00],\n",
       "       [-4.11490356e-01,  8.79371618e-01],\n",
       "       [-4.68405332e-02,  8.49720955e-01],\n",
       "       [-3.59410594e-01,  5.44377223e-01],\n",
       "       [-9.78673484e-02,  1.52980385e+00],\n",
       "       [ 5.98515358e-01,  1.53450944e+00],\n",
       "       [-3.03849032e-01,  2.56286522e+00],\n",
       "       [-9.17981408e-02,  2.05646651e+00],\n",
       "       [ 2.75409174e-02,  2.91070109e-01],\n",
       "       [-1.63712712e+00,  4.62041257e-02],\n",
       "       [-3.63819288e-01,  1.15933621e-01],\n",
       "       [-1.58005970e-01,  6.13109771e-01],\n",
       "       [ 1.58143597e-01,  1.44091333e+00],\n",
       "       [ 7.34230650e-01,  1.54080571e+00],\n",
       "       [-5.54704991e-01,  6.66807917e-01],\n",
       "       [-8.22842147e-02,  1.21357549e+00],\n",
       "       [-2.58503598e-01,  4.56317149e-02],\n",
       "       [ 1.34134794e-01,  2.00529890e-01],\n",
       "       [ 4.09322585e-02,  1.29315181e+00],\n",
       "       [-7.35296050e-02,  6.25980254e-01],\n",
       "       [-6.14240497e-01,  7.56417912e-01],\n",
       "       [-1.85225109e-02,  2.44879200e-01],\n",
       "       [ 1.61591584e+00,  1.21259488e+00],\n",
       "       [-3.19088966e-01,  5.47476250e-01],\n",
       "       [ 3.72178579e-01,  9.13205758e-01],\n",
       "       [-2.81375522e+00,  1.17251445e+00],\n",
       "       [ 3.67994673e-01,  5.56161231e-01],\n",
       "       [ 3.90676848e-01,  1.06603114e+00],\n",
       "       [-5.17928197e-01,  2.30857904e-01],\n",
       "       [ 2.11982625e-01,  1.30070073e+00],\n",
       "       [-1.69034071e-01,  9.49880412e-01],\n",
       "       [-1.43218441e-01,  2.13076981e+00],\n",
       "       [-2.03203689e+00, -1.69536863e-01],\n",
       "       [-6.09696967e-01, -9.92376421e-03],\n",
       "       [-1.01544541e+00,  5.72067227e-01],\n",
       "       [-2.92321169e-01,  2.58299086e+00],\n",
       "       [-8.20097202e-02,  3.26134895e-01],\n",
       "       [-3.81646920e-01,  1.82133648e-01],\n",
       "       [ 4.77352860e-01,  1.09511141e+00],\n",
       "       [ 5.69400930e-01,  3.59837814e-01],\n",
       "       [ 1.04163285e-01,  2.11795779e-01],\n",
       "       [-1.90434997e-01,  5.24070680e-01],\n",
       "       [ 1.07929572e-01,  8.98021923e-01],\n",
       "       [-6.59514493e-01,  1.06500968e+00],\n",
       "       [-4.53142053e-01,  1.06936674e-01],\n",
       "       [ 9.91619314e-01,  2.12627238e+00],\n",
       "       [ 2.64360181e-01,  8.17107939e-01],\n",
       "       [-3.12109923e-01,  2.04744807e-02],\n",
       "       [-4.83220494e-01,  2.59849059e-01],\n",
       "       [-3.94566947e-01,  8.65703153e-01],\n",
       "       [ 6.36279594e-02,  7.25480722e-01],\n",
       "       [-2.50938870e-01,  5.19181513e-01],\n",
       "       [-1.04820127e-01,  5.70679916e-01],\n",
       "       [ 1.39754259e-01,  1.59429649e+00],\n",
       "       [ 3.08540387e-01,  5.43692834e-01],\n",
       "       [-7.94465914e-02,  4.90110178e+00],\n",
       "       [ 1.51943049e-02,  1.34905951e+00],\n",
       "       [-3.15537589e-01,  1.10051719e-01],\n",
       "       [-1.96577840e-01,  1.44674788e+00],\n",
       "       [-2.21774197e+00,  1.17209644e+00],\n",
       "       [ 2.00155389e-02,  1.10841866e+00],\n",
       "       [-5.18908547e-02,  3.56609183e-01],\n",
       "       [-3.42462758e-01, -3.79237520e-02],\n",
       "       [ 5.15432863e-01,  1.58017385e+00],\n",
       "       [-3.92165130e-01,  1.12275653e-01],\n",
       "       [-1.08022979e+00, -3.20989140e-01],\n",
       "       [ 4.05489949e-02,  1.65799653e+00],\n",
       "       [-2.90392712e-02,  3.67066277e-01],\n",
       "       [-2.10809301e+00,  1.28643498e+00],\n",
       "       [ 1.10772193e-01,  9.08157972e-02],\n",
       "       [-2.48670903e+00,  4.24295688e-01],\n",
       "       [-5.61338686e-01,  1.16652564e-01],\n",
       "       [-2.22926320e+00,  5.36486992e-01],\n",
       "       [-1.68433678e+00,  6.84674292e-01],\n",
       "       [-4.65651167e-01,  8.49325852e-01],\n",
       "       [-6.25688087e-01,  1.27678645e+00],\n",
       "       [-5.49483762e-01, -1.75552954e-01],\n",
       "       [ 1.48113963e-01,  8.29060439e-01],\n",
       "       [-2.24968197e+00,  4.25018911e-01],\n",
       "       [-7.87455156e-01,  1.60808883e-02],\n",
       "       [ 1.16469948e-02,  1.62991641e+00],\n",
       "       [-1.22374211e-01,  6.83206662e+00],\n",
       "       [-1.77614920e+00,  5.43123055e-02],\n",
       "       [-2.96217773e-01,  1.86221196e-01],\n",
       "       [-1.46178964e+00,  1.03573771e+00],\n",
       "       [-2.88302587e-01,  2.71841240e-01],\n",
       "       [ 2.68966732e-02,  6.22040517e-01],\n",
       "       [ 3.99662417e-01,  3.53769015e-01],\n",
       "       [-1.44352473e+00,  5.99035230e-01],\n",
       "       [-4.04844741e-01,  3.99300489e-02],\n",
       "       [-1.16525079e-01,  2.82140579e-01],\n",
       "       [-3.89254346e-02,  6.15038005e-01],\n",
       "       [-2.75291131e-01,  6.30225194e+00],\n",
       "       [-4.45842928e-01, -5.58776516e-02],\n",
       "       [ 5.81764984e-01,  1.73418524e+00],\n",
       "       [-4.02465595e-01,  2.59456939e-02],\n",
       "       [-1.83065335e-01,  2.47576788e-01],\n",
       "       [-4.14330276e-01,  9.93119068e-01],\n",
       "       [-1.97752298e-02,  1.04795859e-01],\n",
       "       [-8.93403636e-02,  9.83347413e-02],\n",
       "       [ 3.77130582e-01,  8.32247395e-01],\n",
       "       [ 4.54418229e-01,  1.32906491e+00],\n",
       "       [-1.90860523e-01,  7.52969134e-01],\n",
       "       [-9.97277677e-01,  9.44779724e-01],\n",
       "       [-4.07621946e-01,  6.71087857e-02],\n",
       "       [-1.37337895e-01,  8.22729662e-02],\n",
       "       [ 1.11041076e+00,  1.32442351e+00],\n",
       "       [-4.32067919e-01,  4.66982191e+00],\n",
       "       [ 4.69483311e-01,  8.55739273e-01],\n",
       "       [-3.77428072e-01,  3.99587074e-01],\n",
       "       [-6.40622910e-02, -4.43937476e-02],\n",
       "       [ 6.04450090e-01,  1.72469432e+00],\n",
       "       [-2.27951860e+00,  5.23572222e-01],\n",
       "       [-2.07534404e-01,  3.51775207e-01],\n",
       "       [ 4.95831174e-01,  1.63039301e+00],\n",
       "       [-4.02874707e-01, -1.85103780e-01],\n",
       "       [-2.57037428e-01,  4.19319216e+00],\n",
       "       [-3.66257550e-01,  9.40617257e-01],\n",
       "       [-4.62072212e-01,  1.40336381e+00],\n",
       "       [-8.10884698e+00, -5.70818271e-01],\n",
       "       [ 1.72721926e-01,  2.99568541e-01],\n",
       "       [-8.29362844e-02,  9.60869207e-02],\n",
       "       [-1.38398457e-02,  1.59617464e-01],\n",
       "       [-3.21515410e-01,  3.32560299e-01],\n",
       "       [ 2.09239215e-01,  1.68898496e+00],\n",
       "       [-2.09526562e-01,  3.65184076e-01],\n",
       "       [-7.01191985e-03,  1.80152848e+00],\n",
       "       [-8.27422358e-03,  5.03764144e-01],\n",
       "       [ 1.74877223e-01,  9.77581575e-01],\n",
       "       [ 1.24083856e+00,  1.03213955e+00],\n",
       "       [-1.02645084e-01,  6.81008502e-01],\n",
       "       [-6.02865828e-01, -2.67235778e-01],\n",
       "       [-2.85175619e-01, -1.01818968e-01],\n",
       "       [ 2.74559909e-03,  3.61818876e-01],\n",
       "       [-3.10447163e-01,  1.40724092e-01],\n",
       "       [ 6.61078106e-01,  8.62783066e-01],\n",
       "       [-6.19620724e-02,  3.33075652e-01],\n",
       "       [-3.11378271e-01,  1.98021240e-01],\n",
       "       [ 7.07172519e-01,  1.25963935e+00],\n",
       "       [-5.33429509e-01,  1.33699236e+00],\n",
       "       [-1.85910561e-01,  4.30253415e-01],\n",
       "       [ 5.08203792e-02,  7.19617673e-01],\n",
       "       [-5.19920280e-01,  7.99164035e-01],\n",
       "       [-6.95592572e-01,  3.78432375e-01],\n",
       "       [ 8.61218650e-01,  1.63993096e+00],\n",
       "       [-1.76156852e-01,  1.64088510e+00],\n",
       "       [-4.92150126e-01,  1.31377031e+00],\n",
       "       [-2.80469282e-01,  7.65579017e-01],\n",
       "       [-3.74505337e+00, -1.37904617e+00],\n",
       "       [-1.37670638e-01,  1.66102018e-01],\n",
       "       [-6.73305464e-01,  7.23173626e-01],\n",
       "       [-6.49111922e-02,  3.70221091e-01],\n",
       "       [-5.74799390e-01,  1.10545434e+00],\n",
       "       [ 1.05428428e-01,  7.13446585e-01],\n",
       "       [-1.99413017e-01,  7.84276437e-01],\n",
       "       [-2.40714650e+00,  6.96235968e-01],\n",
       "       [-7.25110990e-01,  2.66121304e+00],\n",
       "       [-1.60941638e+00,  1.30578188e+00],\n",
       "       [-8.44623024e-01, -3.06608870e-01],\n",
       "       [-2.69518438e-01, -2.09458010e-01],\n",
       "       [ 4.16986991e-02,  1.17814302e+00],\n",
       "       [-5.56995549e-01, -7.70174446e-02],\n",
       "       [-4.36908429e-01, -3.33189809e-02],\n",
       "       [-2.05907931e+00,  1.03545060e+00],\n",
       "       [-3.28585352e-01,  5.50674693e-01],\n",
       "       [-1.07400595e-01,  8.70219763e-01],\n",
       "       [-2.52673489e+00,  1.16767893e+00],\n",
       "       [-1.11142655e-01,  5.89861698e-01],\n",
       "       [ 3.90267933e-01,  1.14285515e+00],\n",
       "       [-2.24233622e-01,  1.08548345e-01],\n",
       "       [-5.83133568e-02,  3.45652728e-01],\n",
       "       [-2.71458909e-03,  5.22600361e-01],\n",
       "       [ 3.52335434e-02,  9.92170769e-01],\n",
       "       [ 1.18771732e+00,  6.45496967e-01],\n",
       "       [-3.59801318e-01,  6.63572076e-01],\n",
       "       [-2.41980321e+00,  1.40305027e-01],\n",
       "       [-3.42733347e-01,  4.01034974e-01],\n",
       "       [-2.26937155e-01,  2.47456603e-01],\n",
       "       [ 1.98443608e-01,  2.64959845e+00],\n",
       "       [-2.46594354e+00, -7.69087523e-01],\n",
       "       [-5.86231245e-01,  8.48509391e-01],\n",
       "       [-3.76175227e+00,  1.37112644e-01],\n",
       "       [-1.48806538e-01,  2.77808584e-01],\n",
       "       [-2.51926085e-02,  4.13881851e-01],\n",
       "       [-4.67918808e-01,  3.10387055e-01],\n",
       "       [-1.22937176e-01,  3.71498504e-01],\n",
       "       [-4.84277171e-01,  6.03217171e-01],\n",
       "       [-5.07334179e-01,  5.69349719e-02],\n",
       "       [-8.22428546e+00, -7.21111800e-01],\n",
       "       [-4.46324050e+00, -2.44760520e-01],\n",
       "       [-3.58757011e-01,  1.57421655e-01],\n",
       "       [ 9.60082485e-02,  2.05039237e-01],\n",
       "       [-1.25877002e-01,  2.48177119e-01],\n",
       "       [-2.78053097e+00,  2.34733938e-01],\n",
       "       [-6.07252369e-01,  8.32862866e-01],\n",
       "       [ 1.95512027e-01,  9.90123547e-01],\n",
       "       [-2.86711563e-01,  3.16648789e-01],\n",
       "       [ 1.51324354e-01,  1.08637979e+00],\n",
       "       [-2.18824623e+00,  1.26965658e-01],\n",
       "       [-4.88631749e-01,  9.12355599e-01],\n",
       "       [ 6.33553482e-01,  1.61302662e+00],\n",
       "       [-7.89223050e-02,  6.86877683e-01],\n",
       "       [ 3.59919868e-01,  1.29521686e+00],\n",
       "       [ 8.82577747e-01,  5.45090030e-01],\n",
       "       [-2.11264265e+00,  1.61019573e+00],\n",
       "       [-6.54387193e-01,  1.84584453e+00],\n",
       "       [-1.37193634e+00,  6.16883220e-01],\n",
       "       [-2.84559153e-01,  7.40754047e-01],\n",
       "       [-3.82941263e-01, -1.42159558e-02],\n",
       "       [-3.42641685e-01,  1.57860359e+00],\n",
       "       [ 4.48410345e-01,  1.19908670e+00],\n",
       "       [-1.45362980e-01,  2.37778968e+00],\n",
       "       [-9.65943470e-01,  6.21693927e-01],\n",
       "       [ 5.77476995e-02,  1.32509410e+00],\n",
       "       [-7.33983158e-01,  2.20483851e+00],\n",
       "       [ 6.41381710e-01,  9.98227749e-01],\n",
       "       [-5.74211761e-02,  9.37355766e-02],\n",
       "       [-1.53340446e-01,  6.32716975e-01],\n",
       "       [-1.03304223e-01,  6.96769573e-01],\n",
       "       [-4.56195754e-01,  2.89235973e-01],\n",
       "       [-1.67659017e-01,  3.80520340e-01],\n",
       "       [-2.49399843e-01,  9.21539114e-01],\n",
       "       [-3.29613957e-01,  2.25462941e+00],\n",
       "       [ 6.46772891e-02,  8.07128244e-01],\n",
       "       [-3.00005035e+00, -9.72690548e-01],\n",
       "       [ 1.25122006e+00,  1.00990333e+00],\n",
       "       [-2.81161778e+00,  3.70408926e-01],\n",
       "       [-3.78603008e-01, -2.35971819e-01],\n",
       "       [-2.05191767e+00,  6.10191737e-01],\n",
       "       [-9.32393168e-01,  2.68460238e-01],\n",
       "       [-2.59815944e-01,  1.13761377e+00],\n",
       "       [-1.62109697e-01, -4.66885921e-02],\n",
       "       [-4.96113899e-01,  3.76957717e-01],\n",
       "       [ 3.56480414e-01,  9.11309247e-01],\n",
       "       [-2.92186411e+00,  7.00185047e-01],\n",
       "       [-9.60715443e-01,  9.34579884e-01],\n",
       "       [-3.52809383e-01,  1.51670205e+00],\n",
       "       [-3.81213288e-01, -1.14859135e-01],\n",
       "       [-1.98850072e-01, -6.10716276e-02],\n",
       "       [-4.29237784e+00, -2.13803417e-01],\n",
       "       [-2.08498017e+00, -2.32943275e-01],\n",
       "       [-6.22620802e-01,  6.82752071e-01],\n",
       "       [-9.51578924e-01, -1.67289016e-01],\n",
       "       [ 1.20012229e-01,  2.89305534e-01],\n",
       "       [-8.45999195e-01,  1.56282306e+00],\n",
       "       [-6.10413130e-01,  1.29034917e+00],\n",
       "       [-1.76954679e-01,  2.30469313e-02],\n",
       "       [-4.04762509e-01,  1.24826925e-01],\n",
       "       [-4.29526267e-01,  5.21697154e-01],\n",
       "       [ 2.05005916e-01,  2.89670796e-01],\n",
       "       [-3.36686935e-01,  1.21535734e-01],\n",
       "       [-1.31931063e-01, -1.72687308e-02],\n",
       "       [-1.38480225e-01,  1.97979818e-01],\n",
       "       [-2.20938715e-01,  1.15395083e+00],\n",
       "       [-1.38365313e-01,  2.01211949e+00],\n",
       "       [-2.84008073e-01,  6.28691080e-02],\n",
       "       [ 1.54901513e-01,  2.42104953e-01],\n",
       "       [-3.70987998e-01,  2.39906266e+00],\n",
       "       [ 1.04601452e+00,  6.40313647e-01],\n",
       "       [-1.46148366e+00, -6.39644560e-01],\n",
       "       [ 1.01481250e+00,  1.98654381e+00],\n",
       "       [-5.70213835e-01, -3.42102488e-01],\n",
       "       [-8.17570998e-01,  3.77583897e-01],\n",
       "       [-3.11539416e-01,  1.29155651e+00],\n",
       "       [-7.40786499e-01,  8.09220269e-01],\n",
       "       [ 2.30343308e-01,  3.88315524e-01],\n",
       "       [ 2.03587334e-01,  1.36967003e+00],\n",
       "       [ 2.13715404e-01,  5.05998746e-01],\n",
       "       [-3.39521745e+00,  1.28103059e-01],\n",
       "       [ 3.56330742e-01,  1.07556914e+00],\n",
       "       [ 1.61658322e-01,  2.20737056e+00],\n",
       "       [-2.44282635e+00,  6.71385469e-01],\n",
       "       [ 2.05226014e-01,  3.56168381e+00],\n",
       "       [ 1.09363549e+00,  8.57552581e-01],\n",
       "       [-4.76429545e-01,  1.07200990e-01],\n",
       "       [-5.02466063e-01,  4.44811785e-01],\n",
       "       [-4.05269691e-02,  1.72826425e+00],\n",
       "       [-1.32597060e-01,  6.14820711e-01],\n",
       "       [-6.54974188e-01,  1.98130211e+00],\n",
       "       [ 2.62415801e-01,  1.30508315e+00],\n",
       "       [ 7.27852080e-01,  7.68270379e-01],\n",
       "       [-1.96591350e-01,  5.47634337e+00],\n",
       "       [-6.09426575e-02, -1.30138078e-01],\n",
       "       [-2.24560853e-01,  2.09811347e-01],\n",
       "       [-1.63920304e+00,  5.16197290e-01],\n",
       "       [ 2.16608361e-01,  1.39476367e+00],\n",
       "       [-3.31521102e-01,  2.29400555e+00],\n",
       "       [-3.77471634e-01,  3.25216998e-01],\n",
       "       [ 6.22787149e-01,  1.53939567e+00],\n",
       "       [-2.71087945e-01,  9.92355172e-01],\n",
       "       [-1.27457744e+00,  3.36999126e-02],\n",
       "       [-1.79670972e+00,  2.09204969e-01],\n",
       "       [ 2.43295726e-01,  8.11567576e-01],\n",
       "       [-4.54738474e-01,  7.59748308e-01],\n",
       "       [ 6.17869406e-02,  7.46486730e-01],\n",
       "       [-3.69503362e-01,  8.32594524e-01],\n",
       "       [-9.71783565e-01,  3.21729297e-01],\n",
       "       [ 8.75840292e-01,  1.33886544e+00],\n",
       "       [-1.17880295e-01,  6.71142126e-01],\n",
       "       [ 1.24793000e+00,  1.42961319e+00],\n",
       "       [ 3.86840223e-02,  1.28506079e-01],\n",
       "       [ 1.34643926e-01,  1.74669559e+00],\n",
       "       [-1.80896991e+00, -2.98526954e-01],\n",
       "       [ 4.47581675e-01,  1.43779248e+00],\n",
       "       [-3.62958706e+00, -1.60776909e-01],\n",
       "       [ 3.10495468e-02,  7.08080538e-01],\n",
       "       [ 6.87419419e-01,  9.54796064e-01],\n",
       "       [ 1.94505262e-01,  1.14478099e+00],\n",
       "       [-7.36638580e-01,  4.63425669e-01],\n",
       "       [ 7.08786334e-02,  1.74564853e-01],\n",
       "       [-3.63793258e-01,  2.79534497e+00],\n",
       "       [-9.24780401e-01,  2.99325469e-01],\n",
       "       [ 9.84905319e-02,  5.54265142e-01],\n",
       "       [-4.28225581e-01,  3.01065862e+00],\n",
       "       [-1.14054798e-02,  3.36962447e-01],\n",
       "       [ 5.72174352e-01,  1.76160925e+00],\n",
       "       [ 7.04626259e-01,  1.26419188e+00],\n",
       "       [-1.49581438e+00,  5.18711896e-01],\n",
       "       [-4.21539353e-01, -2.25467231e-02],\n",
       "       [ 7.10651519e-01,  1.41175380e+00],\n",
       "       [ 3.93618726e-01,  8.44760830e-01],\n",
       "       [ 8.98955918e-02,  1.58459979e-01],\n",
       "       [-1.66998308e-01, -4.98055409e-02],\n",
       "       [-1.81821512e-01,  3.94718530e-01],\n",
       "       [ 2.91935645e-01,  1.09488617e+00],\n",
       "       [ 1.85413177e-01,  3.33695260e+00],\n",
       "       [-2.55671011e-01,  8.36327909e-02],\n",
       "       [-3.78454665e-01, -3.84235888e-02],\n",
       "       [-7.67045204e-01, -2.09369224e-01],\n",
       "       [ 4.71185528e-01,  4.25450056e-01],\n",
       "       [-7.63245983e-01, -3.90894259e-01],\n",
       "       [ 7.02234052e-01,  1.29623329e+00],\n",
       "       [ 4.81842603e-01,  4.92924470e+00],\n",
       "       [ 9.48779393e-02,  3.00416679e-01],\n",
       "       [-8.46882345e-01, -4.25289930e-01],\n",
       "       [-1.53913874e-01,  4.25633132e-02],\n",
       "       [ 1.26161628e-01,  3.49091339e-01],\n",
       "       [-3.27851117e-01,  2.82154702e-01],\n",
       "       [-2.50654856e-01,  3.05765874e-01],\n",
       "       [-8.63003930e-01,  3.89736178e-01],\n",
       "       [-7.57457127e-02,  1.02502755e+00],\n",
       "       [-4.82033954e-01,  1.69942703e+00],\n",
       "       [ 2.84072673e-01,  7.25121520e-01],\n",
       "       [ 1.08199490e-01,  9.09414536e-01]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b3d5fbf4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/400\n",
      "7/7 [==============================] - 1s 70ms/step - loss: 1.6128 - mse: 1.6128 - mae: 0.7477 - val_loss: 1.6454 - val_mse: 1.6454 - val_mae: 0.7559\n",
      "Epoch 2/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.4350 - mse: 1.4350 - mae: 0.7144 - val_loss: 1.6190 - val_mse: 1.6190 - val_mae: 0.7407\n",
      "Epoch 3/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1906 - mse: 1.1906 - mae: 0.6656 - val_loss: 1.5975 - val_mse: 1.5975 - val_mae: 0.7270\n",
      "Epoch 4/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.1835 - mse: 1.1835 - mae: 0.6675 - val_loss: 1.5794 - val_mse: 1.5794 - val_mae: 0.7149\n",
      "Epoch 5/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.4563 - mse: 1.4563 - mae: 0.6846 - val_loss: 1.5640 - val_mse: 1.5640 - val_mae: 0.7043\n",
      "Epoch 6/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.3553 - mse: 1.3553 - mae: 0.6580 - val_loss: 1.5510 - val_mse: 1.5510 - val_mae: 0.6954\n",
      "Epoch 7/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2001 - mse: 1.2001 - mae: 0.6392 - val_loss: 1.5407 - val_mse: 1.5407 - val_mae: 0.6877\n",
      "Epoch 8/400\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.0697 - mse: 1.0697 - mae: 0.6041 - val_loss: 1.5322 - val_mse: 1.5322 - val_mae: 0.6810\n",
      "Epoch 9/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.2131 - mse: 1.2131 - mae: 0.6316 - val_loss: 1.5252 - val_mse: 1.5252 - val_mae: 0.6753\n",
      "Epoch 10/400\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 1.3294 - mse: 1.3294 - mae: 0.6199 - val_loss: 1.5192 - val_mse: 1.5192 - val_mae: 0.6707\n",
      "Epoch 11/400\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 1.1855 - mse: 1.1855 - mae: 0.6244 - val_loss: 1.5138 - val_mse: 1.5138 - val_mae: 0.6664\n",
      "Epoch 12/400\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 1.2800 - mse: 1.2800 - mae: 0.6163 - val_loss: 1.5092 - val_mse: 1.5092 - val_mae: 0.6627\n",
      "Epoch 13/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.2977 - mse: 1.2977 - mae: 0.6191 - val_loss: 1.5054 - val_mse: 1.5054 - val_mae: 0.6597\n",
      "Epoch 14/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.2291 - mse: 1.2291 - mae: 0.6142 - val_loss: 1.5020 - val_mse: 1.5020 - val_mae: 0.6569\n",
      "Epoch 15/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1351 - mse: 1.1351 - mae: 0.5880 - val_loss: 1.4990 - val_mse: 1.4990 - val_mae: 0.6545\n",
      "Epoch 16/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2231 - mse: 1.2231 - mae: 0.5919 - val_loss: 1.4965 - val_mse: 1.4965 - val_mae: 0.6524\n",
      "Epoch 17/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.3569 - mse: 1.3569 - mae: 0.6137 - val_loss: 1.4942 - val_mse: 1.4942 - val_mae: 0.6504\n",
      "Epoch 18/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.3377 - mse: 1.3377 - mae: 0.6078 - val_loss: 1.4921 - val_mse: 1.4921 - val_mae: 0.6487\n",
      "Epoch 19/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.3222 - mse: 1.3222 - mae: 0.6294 - val_loss: 1.4903 - val_mse: 1.4903 - val_mae: 0.6472\n",
      "Epoch 20/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.2883 - mse: 1.2883 - mae: 0.6229 - val_loss: 1.4885 - val_mse: 1.4885 - val_mae: 0.6458\n",
      "Epoch 21/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2486 - mse: 1.2486 - mae: 0.6072 - val_loss: 1.4870 - val_mse: 1.4870 - val_mae: 0.6446\n",
      "Epoch 22/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.4006 - mse: 1.4006 - mae: 0.6214 - val_loss: 1.4856 - val_mse: 1.4856 - val_mae: 0.6434\n",
      "Epoch 23/400\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.2677 - mse: 1.2677 - mae: 0.5948 - val_loss: 1.4845 - val_mse: 1.4845 - val_mae: 0.6425\n",
      "Epoch 24/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.2433 - mse: 1.2433 - mae: 0.5720 - val_loss: 1.4834 - val_mse: 1.4834 - val_mae: 0.6416\n",
      "Epoch 25/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.2497 - mse: 1.2497 - mae: 0.5963 - val_loss: 1.4826 - val_mse: 1.4826 - val_mae: 0.6408\n",
      "Epoch 26/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2671 - mse: 1.2671 - mae: 0.6008 - val_loss: 1.4818 - val_mse: 1.4818 - val_mae: 0.6401\n",
      "Epoch 27/400\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.1061 - mse: 1.1061 - mae: 0.5773 - val_loss: 1.4811 - val_mse: 1.4811 - val_mae: 0.6394\n",
      "Epoch 28/400\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 1.1826 - mse: 1.1826 - mae: 0.5827 - val_loss: 1.4805 - val_mse: 1.4805 - val_mae: 0.6387\n",
      "Epoch 29/400\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.2228 - mse: 1.2228 - mae: 0.5699 - val_loss: 1.4799 - val_mse: 1.4799 - val_mae: 0.6381\n",
      "Epoch 30/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1648 - mse: 1.1648 - mae: 0.5817 - val_loss: 1.4793 - val_mse: 1.4793 - val_mae: 0.6374\n",
      "Epoch 31/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2783 - mse: 1.2783 - mae: 0.6021 - val_loss: 1.4787 - val_mse: 1.4787 - val_mae: 0.6368\n",
      "Epoch 32/400\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 1.1524 - mse: 1.1524 - mae: 0.5593 - val_loss: 1.4781 - val_mse: 1.4781 - val_mae: 0.6361\n",
      "Epoch 33/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.3416 - mse: 1.3416 - mae: 0.6084 - val_loss: 1.4775 - val_mse: 1.4775 - val_mae: 0.6355\n",
      "Epoch 34/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1148 - mse: 1.1148 - mae: 0.5667 - val_loss: 1.4770 - val_mse: 1.4770 - val_mae: 0.6349\n",
      "Epoch 35/400\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.1953 - mse: 1.1953 - mae: 0.5752 - val_loss: 1.4766 - val_mse: 1.4766 - val_mae: 0.6344\n",
      "Epoch 36/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.2911 - mse: 1.2911 - mae: 0.5916 - val_loss: 1.4761 - val_mse: 1.4761 - val_mae: 0.6340\n",
      "Epoch 37/400\n",
      "7/7 [==============================] - 0s 41ms/step - loss: 1.2376 - mse: 1.2376 - mae: 0.5731 - val_loss: 1.4756 - val_mse: 1.4756 - val_mae: 0.6334\n",
      "Epoch 38/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.0779 - mse: 1.0779 - mae: 0.5585 - val_loss: 1.4751 - val_mse: 1.4751 - val_mae: 0.6329\n",
      "Epoch 39/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1235 - mse: 1.1235 - mae: 0.5719 - val_loss: 1.4747 - val_mse: 1.4747 - val_mae: 0.6325\n",
      "Epoch 40/400\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.3947 - mse: 1.3947 - mae: 0.6014 - val_loss: 1.4743 - val_mse: 1.4743 - val_mae: 0.6322\n",
      "Epoch 41/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2669 - mse: 1.2669 - mae: 0.5948 - val_loss: 1.4740 - val_mse: 1.4740 - val_mae: 0.6318\n",
      "Epoch 42/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.2325 - mse: 1.2325 - mae: 0.5682 - val_loss: 1.4737 - val_mse: 1.4737 - val_mae: 0.6315\n",
      "Epoch 43/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2712 - mse: 1.2712 - mae: 0.5828 - val_loss: 1.4735 - val_mse: 1.4735 - val_mae: 0.6312\n",
      "Epoch 44/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2231 - mse: 1.2231 - mae: 0.5808 - val_loss: 1.4732 - val_mse: 1.4732 - val_mae: 0.6310\n",
      "Epoch 45/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2302 - mse: 1.2302 - mae: 0.5894 - val_loss: 1.4729 - val_mse: 1.4729 - val_mae: 0.6307\n",
      "Epoch 46/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1139 - mse: 1.1139 - mae: 0.5606 - val_loss: 1.4726 - val_mse: 1.4726 - val_mae: 0.6304\n",
      "Epoch 47/400\n",
      "7/7 [==============================] - 0s 43ms/step - loss: 0.9582 - mse: 0.9582 - mae: 0.5438 - val_loss: 1.4723 - val_mse: 1.4723 - val_mae: 0.6302\n",
      "Epoch 48/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2854 - mse: 1.2854 - mae: 0.5989 - val_loss: 1.4720 - val_mse: 1.4720 - val_mae: 0.6299\n",
      "Epoch 49/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.3358 - mse: 1.3358 - mae: 0.6000 - val_loss: 1.4717 - val_mse: 1.4717 - val_mae: 0.6297\n",
      "Epoch 50/400\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.0450 - mse: 1.0450 - mae: 0.5624 - val_loss: 1.4716 - val_mse: 1.4716 - val_mae: 0.6296\n",
      "Epoch 51/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1559 - mse: 1.1559 - mae: 0.5660 - val_loss: 1.4714 - val_mse: 1.4714 - val_mae: 0.6294\n",
      "Epoch 52/400\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.0512 - mse: 1.0512 - mae: 0.5432 - val_loss: 1.4713 - val_mse: 1.4713 - val_mae: 0.6292\n",
      "Epoch 53/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.3760 - mse: 1.3760 - mae: 0.5914 - val_loss: 1.4711 - val_mse: 1.4711 - val_mae: 0.6290\n",
      "Epoch 54/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3278 - mse: 1.3278 - mae: 0.6127 - val_loss: 1.4709 - val_mse: 1.4709 - val_mae: 0.6288\n",
      "Epoch 55/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2412 - mse: 1.2412 - mae: 0.5800 - val_loss: 1.4708 - val_mse: 1.4708 - val_mae: 0.6286\n",
      "Epoch 56/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1473 - mse: 1.1473 - mae: 0.5769 - val_loss: 1.4706 - val_mse: 1.4706 - val_mae: 0.6284\n",
      "Epoch 57/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.3477 - mse: 1.3477 - mae: 0.5967 - val_loss: 1.4705 - val_mse: 1.4705 - val_mae: 0.6282\n",
      "Epoch 58/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.1639 - mse: 1.1639 - mae: 0.5719 - val_loss: 1.4704 - val_mse: 1.4704 - val_mae: 0.6281\n",
      "Epoch 59/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2428 - mse: 1.2428 - mae: 0.5819 - val_loss: 1.4701 - val_mse: 1.4701 - val_mae: 0.6279\n",
      "Epoch 60/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.2490 - mse: 1.2490 - mae: 0.5784 - val_loss: 1.4699 - val_mse: 1.4699 - val_mae: 0.6277\n",
      "Epoch 61/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.1290 - mse: 1.1290 - mae: 0.5352 - val_loss: 1.4698 - val_mse: 1.4698 - val_mae: 0.6275\n",
      "Epoch 62/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1002 - mse: 1.1002 - mae: 0.5755 - val_loss: 1.4697 - val_mse: 1.4697 - val_mae: 0.6273\n",
      "Epoch 63/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2224 - mse: 1.2224 - mae: 0.5804 - val_loss: 1.4695 - val_mse: 1.4695 - val_mae: 0.6272\n",
      "Epoch 64/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2810 - mse: 1.2810 - mae: 0.5853 - val_loss: 1.4694 - val_mse: 1.4694 - val_mae: 0.6270\n",
      "Epoch 65/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.1179 - mse: 1.1179 - mae: 0.5627 - val_loss: 1.4693 - val_mse: 1.4693 - val_mae: 0.6268\n",
      "Epoch 66/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.2084 - mse: 1.2084 - mae: 0.5735 - val_loss: 1.4693 - val_mse: 1.4693 - val_mae: 0.6267\n",
      "Epoch 67/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.4083 - mse: 1.4083 - mae: 0.5973 - val_loss: 1.4692 - val_mse: 1.4692 - val_mae: 0.6266\n",
      "Epoch 68/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.1052 - mse: 1.1052 - mae: 0.5744 - val_loss: 1.4690 - val_mse: 1.4690 - val_mae: 0.6265\n",
      "Epoch 69/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.2045 - mse: 1.2045 - mae: 0.5715 - val_loss: 1.4689 - val_mse: 1.4689 - val_mae: 0.6263\n",
      "Epoch 70/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.3222 - mse: 1.3222 - mae: 0.5844 - val_loss: 1.4688 - val_mse: 1.4688 - val_mae: 0.6262\n",
      "Epoch 71/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1943 - mse: 1.1943 - mae: 0.5670 - val_loss: 1.4688 - val_mse: 1.4688 - val_mae: 0.6261\n",
      "Epoch 72/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2455 - mse: 1.2455 - mae: 0.5891 - val_loss: 1.4686 - val_mse: 1.4686 - val_mae: 0.6259\n",
      "Epoch 73/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3251 - mse: 1.3251 - mae: 0.6032 - val_loss: 1.4686 - val_mse: 1.4686 - val_mae: 0.6258\n",
      "Epoch 74/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2204 - mse: 1.2204 - mae: 0.5950 - val_loss: 1.4685 - val_mse: 1.4685 - val_mae: 0.6257\n",
      "Epoch 75/400\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.1270 - mse: 1.1270 - mae: 0.5722 - val_loss: 1.4684 - val_mse: 1.4684 - val_mae: 0.6256\n",
      "Epoch 76/400\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 1.3572 - mse: 1.3572 - mae: 0.5956 - val_loss: 1.4683 - val_mse: 1.4683 - val_mae: 0.6255\n",
      "Epoch 77/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1913 - mse: 1.1913 - mae: 0.5836 - val_loss: 1.4683 - val_mse: 1.4683 - val_mae: 0.6254\n",
      "Epoch 78/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2287 - mse: 1.2287 - mae: 0.5809 - val_loss: 1.4681 - val_mse: 1.4681 - val_mae: 0.6252\n",
      "Epoch 79/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1184 - mse: 1.1184 - mae: 0.5569 - val_loss: 1.4680 - val_mse: 1.4680 - val_mae: 0.6251\n",
      "Epoch 80/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.0928 - mse: 1.0928 - mae: 0.5436 - val_loss: 1.4679 - val_mse: 1.4679 - val_mae: 0.6249\n",
      "Epoch 81/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2921 - mse: 1.2921 - mae: 0.5932 - val_loss: 1.4678 - val_mse: 1.4678 - val_mae: 0.6248\n",
      "Epoch 82/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.1228 - mse: 1.1228 - mae: 0.5612 - val_loss: 1.4677 - val_mse: 1.4677 - val_mae: 0.6247\n",
      "Epoch 83/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3306 - mse: 1.3306 - mae: 0.5954 - val_loss: 1.4677 - val_mse: 1.4677 - val_mae: 0.6246\n",
      "Epoch 84/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.9771 - mse: 0.9771 - mae: 0.5402 - val_loss: 1.4677 - val_mse: 1.4677 - val_mae: 0.6245\n",
      "Epoch 85/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.3805 - mse: 1.3805 - mae: 0.5904 - val_loss: 1.4676 - val_mse: 1.4676 - val_mae: 0.6244\n",
      "Epoch 86/400\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.2397 - mse: 1.2397 - mae: 0.5694 - val_loss: 1.4675 - val_mse: 1.4675 - val_mae: 0.6243\n",
      "Epoch 87/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.0467 - mse: 1.0467 - mae: 0.5446 - val_loss: 1.4675 - val_mse: 1.4675 - val_mae: 0.6242\n",
      "Epoch 88/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2544 - mse: 1.2544 - mae: 0.5709 - val_loss: 1.4674 - val_mse: 1.4674 - val_mae: 0.6240\n",
      "Epoch 89/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3465 - mse: 1.3465 - mae: 0.5887 - val_loss: 1.4672 - val_mse: 1.4672 - val_mae: 0.6240\n",
      "Epoch 90/400\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.2589 - mse: 1.2589 - mae: 0.5759 - val_loss: 1.4672 - val_mse: 1.4672 - val_mae: 0.6239\n",
      "Epoch 91/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1711 - mse: 1.1711 - mae: 0.5761 - val_loss: 1.4671 - val_mse: 1.4671 - val_mae: 0.6237\n",
      "Epoch 92/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2740 - mse: 1.2740 - mae: 0.5761 - val_loss: 1.4670 - val_mse: 1.4670 - val_mae: 0.6236\n",
      "Epoch 93/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.0879 - mse: 1.0879 - mae: 0.5476 - val_loss: 1.4669 - val_mse: 1.4669 - val_mae: 0.6233\n",
      "Epoch 94/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.0525 - mse: 1.0525 - mae: 0.5471 - val_loss: 1.4668 - val_mse: 1.4668 - val_mae: 0.6231\n",
      "Epoch 95/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.3977 - mse: 1.3977 - mae: 0.6138 - val_loss: 1.4667 - val_mse: 1.4667 - val_mae: 0.6230\n",
      "Epoch 96/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.0962 - mse: 1.0962 - mae: 0.5626 - val_loss: 1.4660 - val_mse: 1.4660 - val_mae: 0.6222\n",
      "Epoch 97/400\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.1841 - mse: 1.1841 - mae: 0.5760 - val_loss: 1.4650 - val_mse: 1.4650 - val_mae: 0.6211\n",
      "Epoch 98/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.4180 - mse: 1.4180 - mae: 0.6015 - val_loss: 1.4643 - val_mse: 1.4643 - val_mae: 0.6206\n",
      "Epoch 99/400\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.0054 - mse: 1.0054 - mae: 0.5538 - val_loss: 1.4639 - val_mse: 1.4639 - val_mae: 0.6205\n",
      "Epoch 100/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2877 - mse: 1.2877 - mae: 0.5787 - val_loss: 1.4635 - val_mse: 1.4635 - val_mae: 0.6201\n",
      "Epoch 101/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1016 - mse: 1.1016 - mae: 0.5701 - val_loss: 1.4632 - val_mse: 1.4632 - val_mae: 0.6194\n",
      "Epoch 102/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.3226 - mse: 1.3226 - mae: 0.5935 - val_loss: 1.4631 - val_mse: 1.4631 - val_mae: 0.6190\n",
      "Epoch 103/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1621 - mse: 1.1621 - mae: 0.5558 - val_loss: 1.4630 - val_mse: 1.4630 - val_mae: 0.6188\n",
      "Epoch 104/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2971 - mse: 1.2971 - mae: 0.5985 - val_loss: 1.4629 - val_mse: 1.4629 - val_mae: 0.6186\n",
      "Epoch 105/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2451 - mse: 1.2451 - mae: 0.5823 - val_loss: 1.4627 - val_mse: 1.4627 - val_mae: 0.6184\n",
      "Epoch 106/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3420 - mse: 1.3420 - mae: 0.5987 - val_loss: 1.4625 - val_mse: 1.4625 - val_mae: 0.6182\n",
      "Epoch 107/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.0854 - mse: 1.0854 - mae: 0.5462 - val_loss: 1.4624 - val_mse: 1.4624 - val_mae: 0.6180\n",
      "Epoch 108/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.3524 - mse: 1.3524 - mae: 0.5984 - val_loss: 1.4623 - val_mse: 1.4623 - val_mae: 0.6180\n",
      "Epoch 109/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1043 - mse: 1.1043 - mae: 0.5542 - val_loss: 1.4621 - val_mse: 1.4621 - val_mae: 0.6177\n",
      "Epoch 110/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.4187 - mse: 1.4187 - mae: 0.6032 - val_loss: 1.4620 - val_mse: 1.4620 - val_mae: 0.6175\n",
      "Epoch 111/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3081 - mse: 1.3081 - mae: 0.5864 - val_loss: 1.4620 - val_mse: 1.4620 - val_mae: 0.6175\n",
      "Epoch 112/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.2704 - mse: 1.2704 - mae: 0.5833 - val_loss: 1.4619 - val_mse: 1.4619 - val_mae: 0.6174\n",
      "Epoch 113/400\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 1.2739 - mse: 1.2739 - mae: 0.5617 - val_loss: 1.4619 - val_mse: 1.4619 - val_mae: 0.6172\n",
      "Epoch 114/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1521 - mse: 1.1521 - mae: 0.5867 - val_loss: 1.4618 - val_mse: 1.4618 - val_mae: 0.6171\n",
      "Epoch 115/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2681 - mse: 1.2681 - mae: 0.5800 - val_loss: 1.4619 - val_mse: 1.4619 - val_mae: 0.6171\n",
      "Epoch 116/400\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.1393 - mse: 1.1393 - mae: 0.5563 - val_loss: 1.4619 - val_mse: 1.4619 - val_mae: 0.6171\n",
      "Epoch 117/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1805 - mse: 1.1805 - mae: 0.5571 - val_loss: 1.4618 - val_mse: 1.4618 - val_mae: 0.6169\n",
      "Epoch 118/400\n",
      "7/7 [==============================] - 0s 40ms/step - loss: 1.1047 - mse: 1.1047 - mae: 0.5421 - val_loss: 1.4618 - val_mse: 1.4618 - val_mae: 0.6168\n",
      "Epoch 119/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1824 - mse: 1.1824 - mae: 0.5662 - val_loss: 1.4617 - val_mse: 1.4617 - val_mae: 0.6167\n",
      "Epoch 120/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.3692 - mse: 1.3692 - mae: 0.6091 - val_loss: 1.4617 - val_mse: 1.4617 - val_mae: 0.6167\n",
      "Epoch 121/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.1567 - mse: 1.1567 - mae: 0.5523 - val_loss: 1.4617 - val_mse: 1.4617 - val_mae: 0.6166\n",
      "Epoch 122/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2461 - mse: 1.2461 - mae: 0.5803 - val_loss: 1.4617 - val_mse: 1.4617 - val_mae: 0.6166\n",
      "Epoch 123/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2342 - mse: 1.2342 - mae: 0.5736 - val_loss: 1.4616 - val_mse: 1.4616 - val_mae: 0.6166\n",
      "Epoch 124/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.3253 - mse: 1.3253 - mae: 0.5819 - val_loss: 1.4616 - val_mse: 1.4616 - val_mae: 0.6165\n",
      "Epoch 125/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1957 - mse: 1.1957 - mae: 0.5519 - val_loss: 1.4615 - val_mse: 1.4615 - val_mae: 0.6164\n",
      "Epoch 126/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.3964 - mse: 1.3964 - mae: 0.6000 - val_loss: 1.4615 - val_mse: 1.4615 - val_mae: 0.6164\n",
      "Epoch 127/400\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2355 - mse: 1.2355 - mae: 0.5716 - val_loss: 1.4614 - val_mse: 1.4614 - val_mae: 0.6163\n",
      "Epoch 128/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.3275 - mse: 1.3275 - mae: 0.5808 - val_loss: 1.4614 - val_mse: 1.4614 - val_mae: 0.6162\n",
      "Epoch 129/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1651 - mse: 1.1651 - mae: 0.5697 - val_loss: 1.4614 - val_mse: 1.4614 - val_mae: 0.6162\n",
      "Epoch 130/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1663 - mse: 1.1663 - mae: 0.5594 - val_loss: 1.4614 - val_mse: 1.4614 - val_mae: 0.6162\n",
      "Epoch 131/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1133 - mse: 1.1133 - mae: 0.5684 - val_loss: 1.4612 - val_mse: 1.4612 - val_mae: 0.6161\n",
      "Epoch 132/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.3302 - mse: 1.3302 - mae: 0.5933 - val_loss: 1.4612 - val_mse: 1.4612 - val_mae: 0.6161\n",
      "Epoch 133/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.3527 - mse: 1.3527 - mae: 0.5880 - val_loss: 1.4612 - val_mse: 1.4612 - val_mae: 0.6161\n",
      "Epoch 134/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.0257 - mse: 1.0257 - mae: 0.5406 - val_loss: 1.4612 - val_mse: 1.4612 - val_mae: 0.6161\n",
      "Epoch 135/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.2788 - mse: 1.2788 - mae: 0.5665 - val_loss: 1.4611 - val_mse: 1.4611 - val_mae: 0.6160\n",
      "Epoch 136/400\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.2850 - mse: 1.2850 - mae: 0.5595 - val_loss: 1.4610 - val_mse: 1.4610 - val_mae: 0.6160\n",
      "Epoch 137/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.2743 - mse: 1.2743 - mae: 0.5710 - val_loss: 1.4609 - val_mse: 1.4609 - val_mae: 0.6159\n",
      "Epoch 138/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.0878 - mse: 1.0878 - mae: 0.5407 - val_loss: 1.4609 - val_mse: 1.4609 - val_mae: 0.6159\n",
      "Epoch 139/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1326 - mse: 1.1326 - mae: 0.5716 - val_loss: 1.4608 - val_mse: 1.4608 - val_mae: 0.6158\n",
      "Epoch 140/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.4326 - mse: 1.4326 - mae: 0.5875 - val_loss: 1.4607 - val_mse: 1.4607 - val_mae: 0.6157\n",
      "Epoch 141/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1382 - mse: 1.1382 - mae: 0.5580 - val_loss: 1.4607 - val_mse: 1.4607 - val_mae: 0.6156\n",
      "Epoch 142/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1270 - mse: 1.1270 - mae: 0.5417 - val_loss: 1.4607 - val_mse: 1.4607 - val_mae: 0.6157\n",
      "Epoch 143/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.3610 - mse: 1.3610 - mae: 0.5907 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6157\n",
      "Epoch 144/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.2643 - mse: 1.2643 - mae: 0.5729 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 145/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.3130 - mse: 1.3130 - mae: 0.5814 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 146/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.2990 - mse: 1.2990 - mae: 0.5656 - val_loss: 1.4607 - val_mse: 1.4607 - val_mae: 0.6156\n",
      "Epoch 147/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.2296 - mse: 1.2296 - mae: 0.5597 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 148/400\n",
      "7/7 [==============================] - 0s 24ms/step - loss: 1.2146 - mse: 1.2146 - mae: 0.5717 - val_loss: 1.4607 - val_mse: 1.4607 - val_mae: 0.6156\n",
      "Epoch 149/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2506 - mse: 1.2506 - mae: 0.5520 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 150/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.3480 - mse: 1.3480 - mae: 0.6024 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 151/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 27ms/step - loss: 1.3380 - mse: 1.3380 - mae: 0.5673 - val_loss: 1.4606 - val_mse: 1.4606 - val_mae: 0.6156\n",
      "Epoch 152/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.0869 - mse: 1.0869 - mae: 0.5489 - val_loss: 1.4605 - val_mse: 1.4605 - val_mae: 0.6156\n",
      "Epoch 153/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.0693 - mse: 1.0693 - mae: 0.5505 - val_loss: 1.4605 - val_mse: 1.4605 - val_mae: 0.6156\n",
      "Epoch 154/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.2614 - mse: 1.2614 - mae: 0.5813 - val_loss: 1.4605 - val_mse: 1.4605 - val_mae: 0.6156\n",
      "Epoch 155/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2485 - mse: 1.2485 - mae: 0.5597 - val_loss: 1.4605 - val_mse: 1.4605 - val_mae: 0.6156\n",
      "Epoch 156/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.0803 - mse: 1.0803 - mae: 0.5527 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6155\n",
      "Epoch 157/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1112 - mse: 1.1112 - mae: 0.5517 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6155\n",
      "Epoch 158/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.0955 - mse: 1.0955 - mae: 0.5635 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6155\n",
      "Epoch 159/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.1150 - mse: 1.1150 - mae: 0.5358 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6156\n",
      "Epoch 160/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.3739 - mse: 1.3739 - mae: 0.5890 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6155\n",
      "Epoch 161/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1977 - mse: 1.1977 - mae: 0.5621 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6156\n",
      "Epoch 162/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1773 - mse: 1.1773 - mae: 0.5635 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6157\n",
      "Epoch 163/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1067 - mse: 1.1067 - mae: 0.5555 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6157\n",
      "Epoch 164/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.2124 - mse: 1.2124 - mae: 0.5660 - val_loss: 1.4604 - val_mse: 1.4604 - val_mae: 0.6157\n",
      "Epoch 165/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 0.9506 - mse: 0.9506 - mae: 0.5220 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6157\n",
      "Epoch 166/400\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.0251 - mse: 1.0251 - mae: 0.5510 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6157\n",
      "Epoch 167/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3440 - mse: 1.3440 - mae: 0.5906 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6157\n",
      "Epoch 168/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.3047 - mse: 1.3047 - mae: 0.5857 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6157\n",
      "Epoch 169/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.4653 - mse: 1.4653 - mae: 0.6034 - val_loss: 1.4603 - val_mse: 1.4603 - val_mae: 0.6157\n",
      "Epoch 170/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.1269 - mse: 1.1269 - mae: 0.5607 - val_loss: 1.4602 - val_mse: 1.4602 - val_mae: 0.6157\n",
      "Epoch 171/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2963 - mse: 1.2963 - mae: 0.5849 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6156\n",
      "Epoch 172/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1608 - mse: 1.1608 - mae: 0.5739 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6156\n",
      "Epoch 173/400\n",
      "7/7 [==============================] - 0s 42ms/step - loss: 1.3612 - mse: 1.3612 - mae: 0.5806 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6156\n",
      "Epoch 174/400\n",
      "7/7 [==============================] - 0s 38ms/step - loss: 1.2236 - mse: 1.2236 - mae: 0.5563 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6155\n",
      "Epoch 175/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 0.9696 - mse: 0.9696 - mae: 0.5197 - val_loss: 1.4599 - val_mse: 1.4599 - val_mae: 0.6155\n",
      "Epoch 176/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.1467 - mse: 1.1467 - mae: 0.5534 - val_loss: 1.4599 - val_mse: 1.4599 - val_mae: 0.6155\n",
      "Epoch 177/400\n",
      "7/7 [==============================] - 0s 18ms/step - loss: 1.2919 - mse: 1.2919 - mae: 0.5699 - val_loss: 1.4599 - val_mse: 1.4599 - val_mae: 0.6156\n",
      "Epoch 178/400\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.4088 - mse: 1.4088 - mae: 0.6006 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6156\n",
      "Epoch 179/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.3347 - mse: 1.3347 - mae: 0.5853 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 180/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.1773 - mse: 1.1773 - mae: 0.5529 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 181/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.2485 - mse: 1.2485 - mae: 0.5566 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 182/400\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.0833 - mse: 1.0833 - mae: 0.5377 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 183/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.0574 - mse: 1.0574 - mae: 0.5383 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 184/400\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.0703 - mse: 1.0703 - mae: 0.5523 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 185/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.1166 - mse: 1.1166 - mae: 0.5455 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 186/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.3108 - mse: 1.3108 - mae: 0.5900 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6157\n",
      "Epoch 187/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.0601 - mse: 1.0601 - mae: 0.5516 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 188/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.3813 - mse: 1.3813 - mae: 0.5775 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 189/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.4441 - mse: 1.4441 - mae: 0.5764 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 190/400\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.2423 - mse: 1.2423 - mae: 0.5800 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 191/400\n",
      "7/7 [==============================] - 0s 33ms/step - loss: 1.2966 - mse: 1.2966 - mae: 0.5726 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 192/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.4040 - mse: 1.4040 - mae: 0.5841 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6157\n",
      "Epoch 193/400\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.2797 - mse: 1.2797 - mae: 0.5741 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6158\n",
      "Epoch 194/400\n",
      "7/7 [==============================] - 0s 36ms/step - loss: 1.1288 - mse: 1.1288 - mae: 0.5490 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6158\n",
      "Epoch 195/400\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.0749 - mse: 1.0749 - mae: 0.5369 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6158\n",
      "Epoch 196/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.1984 - mse: 1.1984 - mae: 0.5609 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6158\n",
      "Epoch 197/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.0662 - mse: 1.0662 - mae: 0.5563 - val_loss: 1.4599 - val_mse: 1.4599 - val_mae: 0.6157\n",
      "Epoch 198/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2528 - mse: 1.2528 - mae: 0.5628 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6157\n",
      "Epoch 199/400\n",
      "7/7 [==============================] - 0s 22ms/step - loss: 1.0783 - mse: 1.0783 - mae: 0.5405 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6157\n",
      "Epoch 200/400\n",
      "7/7 [==============================] - 0s 29ms/step - loss: 1.2219 - mse: 1.2219 - mae: 0.5650 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 201/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 35ms/step - loss: 1.5034 - mse: 1.5034 - mae: 0.5976 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 202/400\n",
      "7/7 [==============================] - 0s 25ms/step - loss: 1.0868 - mse: 1.0868 - mae: 0.5286 - val_loss: 1.4599 - val_mse: 1.4599 - val_mae: 0.6158\n",
      "Epoch 203/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2678 - mse: 1.2678 - mae: 0.5690 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6159\n",
      "Epoch 204/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.2389 - mse: 1.2389 - mae: 0.5673 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6160\n",
      "Epoch 205/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 1.2141 - mse: 1.2141 - mae: 0.5638 - val_loss: 1.4602 - val_mse: 1.4602 - val_mae: 0.6161\n",
      "Epoch 206/400\n",
      "7/7 [==============================] - 0s 30ms/step - loss: 1.3594 - mse: 1.3594 - mae: 0.6036 - val_loss: 1.4602 - val_mse: 1.4602 - val_mae: 0.6160\n",
      "Epoch 207/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.3232 - mse: 1.3232 - mae: 0.5762 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6160\n",
      "Epoch 208/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.1039 - mse: 1.1039 - mae: 0.5458 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6160\n",
      "Epoch 209/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.3111 - mse: 1.3111 - mae: 0.5843 - val_loss: 1.4601 - val_mse: 1.4601 - val_mae: 0.6160\n",
      "Epoch 210/400\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.2474 - mse: 1.2474 - mae: 0.5679 - val_loss: 1.4600 - val_mse: 1.4600 - val_mae: 0.6159\n",
      "Epoch 211/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.3784 - mse: 1.3784 - mae: 0.5936 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 212/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.0352 - mse: 1.0352 - mae: 0.5410 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 213/400\n",
      "7/7 [==============================] - 0s 32ms/step - loss: 1.1422 - mse: 1.1422 - mae: 0.5405 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6158\n",
      "Epoch 214/400\n",
      "7/7 [==============================] - 0s 19ms/step - loss: 1.4187 - mse: 1.4187 - mae: 0.5939 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 215/400\n",
      "7/7 [==============================] - 0s 23ms/step - loss: 1.2635 - mse: 1.2635 - mae: 0.5719 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 216/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.0981 - mse: 1.0981 - mae: 0.5439 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 217/400\n",
      "7/7 [==============================] - 0s 20ms/step - loss: 1.3678 - mse: 1.3678 - mae: 0.5827 - val_loss: 1.4598 - val_mse: 1.4598 - val_mae: 0.6158\n",
      "Epoch 218/400\n",
      "7/7 [==============================] - 0s 21ms/step - loss: 1.0959 - mse: 1.0959 - mae: 0.5421 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 219/400\n",
      "7/7 [==============================] - 0s 35ms/step - loss: 1.2234 - mse: 1.2234 - mae: 0.5789 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 220/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 0.9424 - mse: 0.9424 - mae: 0.5150 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 221/400\n",
      "7/7 [==============================] - 0s 37ms/step - loss: 1.2425 - mse: 1.2425 - mae: 0.5613 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 222/400\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 1.1550 - mse: 1.1550 - mae: 0.5359 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 223/400\n",
      "7/7 [==============================] - 0s 39ms/step - loss: 1.4363 - mse: 1.4363 - mae: 0.6061 - val_loss: 1.4596 - val_mse: 1.4596 - val_mae: 0.6159\n",
      "Epoch 224/400\n",
      "7/7 [==============================] - 0s 27ms/step - loss: 1.1239 - mse: 1.1239 - mae: 0.5412 - val_loss: 1.4596 - val_mse: 1.4596 - val_mae: 0.6159\n",
      "Epoch 225/400\n",
      "7/7 [==============================] - 0s 26ms/step - loss: 1.1114 - mse: 1.1114 - mae: 0.5347 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6159\n",
      "Epoch 226/400\n",
      "7/7 [==============================] - 0s 28ms/step - loss: 0.9529 - mse: 0.9529 - mae: 0.5137 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6160\n",
      "Epoch 227/400\n",
      "7/7 [==============================] - 0s 34ms/step - loss: 1.1023 - mse: 1.1023 - mae: 0.5441 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6160\n",
      "Epoch 228/400\n",
      "7/7 [==============================] - 0s 31ms/step - loss: 1.1752 - mse: 1.1752 - mae: 0.5509 - val_loss: 1.4597 - val_mse: 1.4597 - val_mae: 0.6160\n",
      "Epoch 229/400\n",
      "6/7 [========================>.....] - ETA: 0s - loss: 1.3705 - mse: 1.3705 - mae: 0.5873"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-43c8f70f63a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     76\u001b[0m     callbacks = [EarlyStopping(monitor='val_loss', patience=100),\n\u001b[1;32m     77\u001b[0m              ModelCheckpoint(filepath='Biv_nonStationary_model.h5', monitor='val_loss', save_best_only=True)]\n\u001b[0;32m---> 78\u001b[0;31m     result = model.fit(x_train1, y_train[:,0],callbacks = callbacks, \n\u001b[0m\u001b[1;32m     79\u001b[0m                        validation_data=(x_test1,y_test[:,0]), epochs = 400, batch_size = 128, verbose = 1)\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "### estimate for LMC\n",
    "## Functions for calculation of MSE and MAE\n",
    "def mse(y_pred,y_true):\n",
    "    mse = np.mean((y_pred-y_true)**2)\n",
    "    return mse\n",
    "\n",
    "def mae(y_pred,y_true):\n",
    "    mae = np.mean(np.absolute(y_pred-y_true))\n",
    "    return mae\n",
    "\n",
    "mse_var1 = []\n",
    "mse_var2 = []\n",
    "for i in range(1):\n",
    "    df_loc = pd.read_csv(\"../synthetic_data_simulations_gaussian/2d_gaussian_1200_\"+str(i+1)+\".csv\", sep = \",\")\n",
    "\n",
    "    N = len(df_loc)\n",
    "    s = np.vstack((df_loc[\"x\"],df_loc[\"y\"])).T\n",
    "    y = np.array(df_loc[[\"var1\",\"var2\"]])\n",
    "    num_basis = [2**2,5**2,9**2]\n",
    "    knots_1d = [np.linspace(0,1,int(np.sqrt(i))) for i in num_basis]\n",
    "    #knots_1d = [np.linspace(0,1,i) for i in num_basis]\n",
    "    ##Wendland kernel\n",
    "    K = 0\n",
    "    phi = np.zeros((N, sum(num_basis)))\n",
    "    for res in range(len(num_basis)):\n",
    "        theta = 1/np.sqrt(num_basis[res])*2.5\n",
    "        knots_s1, knots_s2 = np.meshgrid(knots_1d[res],knots_1d[res])\n",
    "        knots = np.column_stack((knots_s1.flatten(),knots_s2.flatten()))\n",
    "        for i in range(num_basis[res]):\n",
    "            d = np.linalg.norm(s-knots[i,:],axis=1)/theta\n",
    "            for j in range(len(d)):\n",
    "                if d[j] >= 0 and d[j] <= 1:\n",
    "                    phi[j,i + K] = (1-d[j])**6 * (35 * d[j]**2 + 18 * d[j] + 3)/3\n",
    "                else:\n",
    "                    phi[j,i + K] = 0\n",
    "        K = K + num_basis[res]\n",
    "   ### pre-processing (multiply with lmc estimates)\n",
    "\n",
    "    phi1 = 1.28*phi + 0.36*phi   ### variable 1 basis functions\n",
    "    phi2 = 0.639*phi + 0.903*phi ### variable 2 basis functions\n",
    "    N_train = s_train.shape[0]\n",
    "    N_test = s_test.shape[0]\n",
    "    s_train, s_test,x_train1,x_test1,x_train2,x_test2,y_train, y_test= train_test_split(s, phi1, phi2, y, \n",
    "                                                            test_size=0.333)\n",
    "#     data_train = np.hstack((encoder_train,y_train))\n",
    "#     n_rows = data_train.shape[0]\n",
    "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
    "#     resampled_data_train = data_train[random_indices, :]\n",
    "    # DeepKriging model for continuous data\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(1, input_dim = x_train1.shape[1],  \n",
    "                    kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(rate=0.5))\n",
    "    # model.add(BatchNormalization())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "    NB_START_EPOCHS = 50 \n",
    "    # NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "    BATCH_SIZE = 64  \n",
    "    fold_no = 1\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=100),\n",
    "             ModelCheckpoint(filepath='Biv_nonStationary_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "    result = model.fit(x_train1, y_train[:,0],callbacks = callbacks, \n",
    "                       validation_data=(x_test1,y_test[:,0]), epochs = 400, batch_size = 128, verbose = 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     result = model.fit(x_train, y_train, callbacks=callbacks, \n",
    "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
    "    model = keras.models.load_model('Biv_nonStationary_model.h5')\n",
    "    y_pred1 = model.predict(x_test1)\n",
    "    mse_var1.append(mse(y_test[:,0], y_pred1))\n",
    "    \n",
    "    \n",
    "#     data_train = np.hstack((encoder_train,y_train))\n",
    "#     n_rows = data_train.shape[0]\n",
    "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
    "#     resampled_data_train = data_train[random_indices, :]\n",
    "    # DeepKriging model for continuous data\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(1, input_dim = x_train2.shape[1],  \n",
    "                    kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(rate=0.5))\n",
    "    # model.add(BatchNormalization())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "#     model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "#     model.add(Dense(1, activation='linear'))\n",
    "    NB_START_EPOCHS = 50 \n",
    "    # NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "    BATCH_SIZE = 64  \n",
    "    fold_no = 1\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=100),\n",
    "             ModelCheckpoint(filepath='Biv_nonStationary_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "    result = model.fit(x_train2, y_train[:,1],callbacks = callbacks, \n",
    "                       validation_data=(x_test2,y_test[:,1]), epochs = 400, batch_size = 128, verbose = 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     result = model.fit(x_train, y_train, callbacks=callbacks, \n",
    "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
    "    model = keras.models.load_model('Biv_nonStationary_model.h5')\n",
    "    y_pred2 = model.predict(x_test2)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse_var2.append(mse(y_test[:,1], y_pred2))\n",
    "#     mse_var2.append(mean_squared_error(y_test[:,1], y_pred[:,1]))\n",
    "#     print(y_pred1[:,0])\n",
    "#     print(y_pred2[:,0])\n",
    "    correlation.append(np.corrcoef(np.array(y_pred1[:,0]),np.array(y_pred2[:,0]))[0,1])\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ae1e2a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse_var2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f363f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean var 1 :  0.0073751523257748084\n",
      "mean var 2 :  0.00571064356234115\n",
      "variance var 1 :  5.973672588415026e-07\n",
      "variance var 2 :  3.679894352165365e-07\n",
      "correlation :  0.8773736657612876\n"
     ]
    }
   ],
   "source": [
    "### lmc coefficients multiplied \n",
    "print(\"mean var 1 : \",np.mean(mse_var1))\n",
    "print(\"mean var 2 : \",np.mean(mse_var2))\n",
    "print(\"variance var 1 : \",np.var(mse_var1))\n",
    "print(\"variance var 2 : \",np.var(mse_var2))\n",
    "print(\"correlation : \",np.mean(correlation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e7480e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean var 1 :  0.0069509933472384116\n",
      "mean var 2 :  0.0055481564103758065\n",
      "variance var 1 :  2.131914925257068e-07\n",
      "variance var 2 :  3.657565294016452e-07\n",
      "correlation :  0.8698024609977338\n"
     ]
    }
   ],
   "source": [
    "### independent single variable training\n",
    "print(\"mean var 1 : \",np.mean(mse_var1))\n",
    "print(\"mean var 2 : \",np.mean(mse_var2))\n",
    "print(\"variance var 1 : \",np.var(mse_var1))\n",
    "print(\"variance var 2 : \",np.var(mse_var2))\n",
    "print(\"correlation : \",np.mean(correlation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "890d592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean var 1 :  0.1283436222252356\n",
      "mean var 2 :  0.08093607664785937\n",
      "variance var 1 :  0.001727034795057943\n",
      "variance var 2 :  0.0008055564705659079\n",
      "correlation :  0.8722977348119214\n"
     ]
    }
   ],
   "source": [
    "### low rank approximation of lmc\n",
    "print(\"mean var 1 : \",np.mean(mse_var1))\n",
    "print(\"mean var 2 : \",np.mean(mse_var2))\n",
    "print(\"variance var 1 : \",np.var(mse_var1))\n",
    "print(\"variance var 2 : \",np.var(mse_var2))\n",
    "print(\"correlation : \",np.mean(correlation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c70772",
   "metadata": {},
   "source": [
    "## comparison results \n",
    "\n",
    "### Biv_deepKriging\n",
    "- mean var 1 :  0.006534991334629507\n",
    "- mean var 2 :  0.005235456543853871\n",
    "- variance var 1 :  4.2874083303066245e-07\n",
    "- variance var 2 :  1.785717711119858e-07\n",
    "- correlation :  0.8730484697671853\n",
    "\n",
    "### lmc coefficients multiplied \n",
    "- mean var 1 :  0.0073751523257748084 \n",
    "- mean var 2 :  0.00571064356234115\n",
    "- variance var 1 :  5.973672588415026e-07\n",
    "- variance var 2 :  3.679894352165365e-07\n",
    "- correlation :  0.8773736657612876\n",
    "\n",
    "### Independent single variable training\n",
    "- mean var 1 :  0.0069509933472384116\n",
    "- mean var 2 :  0.0055481564103758065\n",
    "- variance var 1 :  2.131914925257068e-07\n",
    "- variance var 2 :  3.657565294016452e-07\n",
    "- correlation :  0.8698024609977338\n",
    "\n",
    "### low rank approximation of lmc\n",
    "- mean var 1 :  0.1283436222252356\n",
    "- mean var 2 :  0.08093607664785937\n",
    "- variance var 1 :  0.001727034795057943\n",
    "- variance var 2 :  0.0008055564705659079\n",
    "- correlation :  0.8722977348119214"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8e6c19da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0056749999999999995"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(0.006144+0.005206)/2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80309f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for i in range(1):\n",
    "    s_train, s_test,x_train,x_test,y_train, y_test= train_test_split(s, phi, y, \n",
    "                                                            test_size=0.05)\n",
    "#     data_train = np.hstack((encoder_train,y_train))\n",
    "#     n_rows = data_train.shape[0]\n",
    "#     random_indices = np.random.choice(n_rows, size=10000, replace=True)\n",
    "#     resampled_data_train = data_train[random_indices, :]\n",
    "    # DeepKriging model for continuous data\n",
    "    model = Sequential()\n",
    "    # model.add(Dense(100, input_dim = 2,  kernel_initializer='he_uniform', activation='relu'))\n",
    "    model.add(Dense(100, input_dim = x_train.shape[1],  \n",
    "                    kernel_initializer='he_uniform', activation='relu'))\n",
    "    # model.add(Dropout(rate=0.5))\n",
    "    # model.add(BatchNormalization())\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "#     model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dense(50, activation='relu'))\n",
    "    #model.add(Dropout(rate=0.5))\n",
    "#     model.add(Dense(10, activation='relu'))\n",
    "    #model.add(BatchNormalization())\n",
    "    model.add(Dense(2, activation='linear'))\n",
    "    NB_START_EPOCHS = 50 \n",
    "    # NB_START_EPOCHS = 200  # Number of epochs we usually start to train with\n",
    "    BATCH_SIZE = 64  \n",
    "    fold_no = 1\n",
    "    optimizer = keras.optimizers.Adam(lr=0.001)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mse','mae'])\n",
    "\n",
    "    callbacks = [EarlyStopping(monitor='val_loss', patience=100),\n",
    "             ModelCheckpoint(filepath='Biv_nonStationary_model.h5', monitor='val_loss', save_best_only=True)]\n",
    "    result = model.fit(x_train, y_train,callbacks = callbacks, \n",
    "                       validation_data=(x_test,y_test), epochs = 400, batch_size = 128, verbose = 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "#     result = model.fit(x_train, y_train, callbacks=callbacks, \n",
    "#                        validation_data=(x_test,y_test), epochs = 200, batch_size = 64, verbose = 2)\n",
    "    model = keras.models.load_model('Biv_nonStationary_model.h5')\n",
    "    y_pred = model.predict(x_test)\n",
    "\n",
    "    # Mean Squared Error\n",
    "    mse_var1.append(mean_squared_error(y_test[:,0], y_pred[:,0]))\n",
    "    mse_var2.append(mean_squared_error(y_test[:,1], y_pred[:,1]))\n",
    "end_time = time.time()\n",
    "print(\"%s seconds\", end_time - start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
